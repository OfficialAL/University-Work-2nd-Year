{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAiKUBVcUgxP"
      },
      "source": [
        "\n",
        "# **Section1: Import Liberaries and dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import essential libraries for data analysis and machine learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load the insurance dataset\n",
        "df = pd.read_csv('insurance.csv')\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzw-NQgLU2b5"
      },
      "source": [
        "# **Section2: Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 2: EXPLORATORY DATA ANALYSIS\n",
        "# Zak/Orlando Instructions: Implement the following analysis steps\n",
        "\n",
        "# 1. BASIC DATA EXPLORATION\n",
        "# TODO: Check for missing values\n",
        "# df.isnull().sum()\n",
        "# TODO: Check data types and basic statistics for key variables (age, bmi, charges)\n",
        "# Focus on the relationship between age, BMI, and insurance charges\n",
        "\n",
        "# 2. DISTRIBUTION ANALYSIS\n",
        "# TODO: Create histograms for key variables:\n",
        "# - Age distribution\n",
        "# - BMI distribution  \n",
        "# - Charges distribution (target variable)\n",
        "# Use plt.figure(figsize=(15, 5)) and create subplots\n",
        "\n",
        "# 3. CORRELATION ANALYSIS\n",
        "# TODO: Calculate correlation matrix focusing on numerical variables\n",
        "# correlation_matrix = df.corr()\n",
        "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "# Pay special attention to correlations with 'charges'\n",
        "\n",
        "# 4. BIAS DETECTION ANALYSIS\n",
        "# TODO: Analyze potential bias in insurance charges based on:\n",
        "# a) Age groups: Create age bins (e.g., 18-30, 31-45, 46-60, 60+)\n",
        "# b) BMI categories: Underweight (<18.5), Normal (18.5-24.9), Overweight (25-29.9), Obese (30+)\n",
        "# c) Create box plots showing charge distribution across these groups\n",
        "\n",
        "# 5. CATEGORICAL VARIABLE ANALYSIS\n",
        "# TODO: Analyze the impact of categorical variables:\n",
        "# - sex: Male vs Female insurance charges\n",
        "# - smoker: Smoker vs Non-smoker (this will likely show the strongest bias)\n",
        "# - region: Regional differences in charges\n",
        "# Use box plots and violin plots\n",
        "\n",
        "# 6. SCATTER PLOTS FOR RELATIONSHIPS\n",
        "# TODO: Create scatter plots to visualize:\n",
        "# - Age vs Charges (color by smoker status)\n",
        "# - BMI vs Charges (color by smoker status)\n",
        "# - Age vs BMI (color by charges - use continuous color scale)\n",
        "\n",
        "# 7. STATISTICAL TESTS\n",
        "# TODO: Perform statistical tests to confirm bias:\n",
        "# - ANOVA test for charges across age groups\n",
        "# - T-test for charges between male/female\n",
        "# - T-test for charges between smokers/non-smokers\n",
        "# from scipy.stats import f_oneway, ttest_ind\n",
        "\n",
        "# 8. SUMMARY INSIGHTS\n",
        "# TODO: Create a summary of key findings:\n",
        "# - Which factors show the strongest correlation with charges?\n",
        "# - Is there evidence of age bias in insurance pricing?\n",
        "# - Is there evidence of BMI bias in insurance pricing?\n",
        "# - What other biases are present in the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg1ecqGBUerz"
      },
      "source": [
        "# **Secion3: Data Preprocessing and Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 3: DATA PREPROCESSING AND CLEANING\n",
        "# Mohammed Instructions: Implement the following preprocessing steps\n",
        "\n",
        "# 1. HANDLE MISSING VALUES\n",
        "# TODO: Based on Section 2 analysis, handle any missing values\n",
        "# Check if any imputation is needed or if rows should be dropped\n",
        "\n",
        "# 2. OUTLIER DETECTION AND TREATMENT\n",
        "# TODO: Identify and handle outliers in key variables:\n",
        "# - Use IQR method or Z-score to detect outliers in 'charges', 'age', 'bmi'\n",
        "# - Decide whether to remove, cap, or transform outliers\n",
        "# - Document the impact of outlier treatment on bias analysis\n",
        "\n",
        "# 3. FEATURE ENCODING\n",
        "# TODO: Encode categorical variables for machine learning:\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# le = LabelEncoder()\n",
        "# \n",
        "# # Encode binary categorical variables\n",
        "# df['sex_encoded'] = le.fit_transform(df['sex'])\n",
        "# df['smoker_encoded'] = le.fit_transform(df['smoker'])\n",
        "# \n",
        "# # For 'region', use one-hot encoding since it's nominal\n",
        "# df_encoded = pd.get_dummies(df, columns=['region'], prefix='region')\n",
        "\n",
        "# 4. FEATURE ENGINEERING\n",
        "# TODO: Create new features that might help with bias analysis:\n",
        "# \n",
        "# # Age groups for bias analysis\n",
        "# def categorize_age(age):\n",
        "#     if age < 30:\n",
        "#         return 'Young'\n",
        "#     elif age < 45:\n",
        "#         return 'Middle-aged'\n",
        "#     elif age < 60:\n",
        "#         return 'Senior'\n",
        "#     else:\n",
        "#         return 'Elderly'\n",
        "# \n",
        "# # BMI categories\n",
        "# def categorize_bmi(bmi):\n",
        "#     if bmi < 18.5:\n",
        "#         return 'Underweight'\n",
        "#     elif bmi < 25:\n",
        "#         return 'Normal'\n",
        "#     elif bmi < 30:\n",
        "#         return 'Overweight'\n",
        "#     else:\n",
        "#         return 'Obese'\n",
        "# \n",
        "# df['age_group'] = df['age'].apply(categorize_age)\n",
        "# df['bmi_category'] = df['bmi'].apply(categorize_bmi)\n",
        "\n",
        "# 5. FEATURE SCALING\n",
        "# TODO: Scale numerical features for machine learning:\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# numerical_features = ['age', 'bmi', 'children']\n",
        "# df_scaled = df.copy()\n",
        "# df_scaled[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "# 6. DATA SPLITTING\n",
        "# TODO: Split the data into training and testing sets:\n",
        "# # Prepare features and target\n",
        "# X = df_scaled.drop(['charges'], axis=1)  # Remove target and non-encoded categorical columns\n",
        "# y = df_scaled['charges']\n",
        "# \n",
        "# # Split the data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# \n",
        "# print(f\"Training set size: {X_train.shape}\")\n",
        "# print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "# 7. BIAS MITIGATION CONSIDERATIONS\n",
        "# TODO: Consider if any bias mitigation techniques are needed:\n",
        "# - Are certain groups under/over-represented in the data?\n",
        "# - Should we balance the dataset by age groups or BMI categories?\n",
        "# - Document any ethical considerations for the model\n",
        "\n",
        "# 8. FINAL DATA VALIDATION\n",
        "# TODO: Validate the preprocessed data:\n",
        "# - Check for any remaining missing values\n",
        "# - Verify all categorical variables are properly encoded\n",
        "# - Ensure no data leakage between train and test sets\n",
        "# - Save the preprocessed data for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2o-8vGUU9gk"
      },
      "source": [
        "# **Section4: Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 4: MODEL TRAINING\n",
        "# Zak/Orlando Instructions: Implement multiple regression models to predict insurance charges\n",
        "\n",
        "# 1. BASELINE MODEL - LINEAR REGRESSION\n",
        "# TODO: Start with a simple linear regression model:\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "# \n",
        "# # Train baseline linear regression\n",
        "# lr_model = LinearRegression()\n",
        "# lr_model.fit(X_train, y_train)\n",
        "# \n",
        "# # Make predictions\n",
        "# y_pred_lr = lr_model.predict(X_test)\n",
        "# \n",
        "# # Evaluate baseline model\n",
        "# lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
        "# lr_r2 = r2_score(y_test, y_pred_lr)\n",
        "# lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
        "\n",
        "# 2. ADVANCED MODEL - RANDOM FOREST\n",
        "# TODO: Train a Random Forest model for comparison:\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# \n",
        "# # Train Random Forest\n",
        "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# rf_model.fit(X_train, y_train)\n",
        "# \n",
        "# # Make predictions\n",
        "# y_pred_rf = rf_model.predict(X_test)\n",
        "# \n",
        "# # Evaluate Random Forest model\n",
        "# rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
        "# rf_r2 = r2_score(y_test, y_pred_rf)\n",
        "# rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
        "\n",
        "# 3. ADDITIONAL MODELS (OPTIONAL)\n",
        "# TODO: Consider implementing additional models:\n",
        "# - Gradient Boosting Regressor\n",
        "# - Support Vector Regression\n",
        "# - Ridge/Lasso Regression for regularization\n",
        "\n",
        "# 4. FEATURE IMPORTANCE ANALYSIS\n",
        "# TODO: Analyze feature importance to understand bias:\n",
        "# # For Random Forest\n",
        "# feature_importance = rf_model.feature_importances_\n",
        "# feature_names = X_train.columns\n",
        "# \n",
        "# # Create a dataframe for better visualization\n",
        "# importance_df = pd.DataFrame({\n",
        "#     'feature': feature_names,\n",
        "#     'importance': feature_importance\n",
        "# }).sort_values('importance', ascending=False)\n",
        "# \n",
        "# # Plot feature importance\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
        "# plt.title('Top 10 Most Important Features')\n",
        "# plt.xlabel('Feature Importance')\n",
        "\n",
        "# 5. BIAS ANALYSIS IN PREDICTIONS\n",
        "# TODO: Analyze if the model perpetuates or reduces bias:\n",
        "# \n",
        "# # Add predictions to test set for analysis\n",
        "# test_analysis = X_test.copy()\n",
        "# test_analysis['actual_charges'] = y_test\n",
        "# test_analysis['predicted_charges'] = y_pred_rf\n",
        "# test_analysis['prediction_error'] = test_analysis['actual_charges'] - test_analysis['predicted_charges']\n",
        "# \n",
        "# # Analyze prediction errors by demographic groups\n",
        "# # Group by age categories and check for systematic bias\n",
        "# # Group by BMI categories and check for systematic bias\n",
        "# # Group by sex and check for systematic bias\n",
        "\n",
        "# 6. MODEL COMPARISON\n",
        "# TODO: Create a comparison table of all models:\n",
        "# model_comparison = pd.DataFrame({\n",
        "#     'Model': ['Linear Regression', 'Random Forest'],\n",
        "#     'MSE': [lr_mse, rf_mse],\n",
        "#     'R²': [lr_r2, rf_r2],\n",
        "#     'MAE': [lr_mae, rf_mae]\n",
        "# })\n",
        "# \n",
        "# print(\"Model Performance Comparison:\")\n",
        "# print(model_comparison)\n",
        "\n",
        "# 7. CROSS-VALIDATION\n",
        "# TODO: Implement cross-validation for more robust evaluation:\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# \n",
        "# # Perform 5-fold cross-validation\n",
        "# cv_scores_lr = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='r2')\n",
        "# cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')\n",
        "# \n",
        "# print(f\"Linear Regression CV R² Score: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std() * 2:.4f})\")\n",
        "# print(f\"Random Forest CV R² Score: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})\")\n",
        "\n",
        "# 8. SAVE TRAINED MODELS\n",
        "# TODO: Save the best performing models for later use:\n",
        "# import joblib\n",
        "# joblib.dump(rf_model, 'best_insurance_model.pkl')\n",
        "# joblib.dump(scaler, 'feature_scaler.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjqkvr3zVHwC"
      },
      "source": [
        "# **Section5: Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 5: MODEL EVALUATION (Alex)\n",
        "# Focus: Comprehensive evaluation of trained models with bias analysis\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =============================================================================\n",
        "# EVALUATION FRAMEWORK - WORKS WITH ANY TRAINED MODELS FROM SECTION 4\n",
        "# =============================================================================\n",
        "\n",
        "def comprehensive_model_evaluation(models_dict, X_test, y_test, model_names=None):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation function for multiple regression models\n",
        "    \n",
        "    Parameters:\n",
        "    models_dict: dict of trained models {model_name: model_object}\n",
        "    X_test, y_test: test data\n",
        "    model_names: optional list of display names\n",
        "    \"\"\"\n",
        "    \n",
        "    results = {}\n",
        "    predictions = {}\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"COMPREHENSIVE MODEL EVALUATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for name, model in models_dict.items():\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test)\n",
        "        predictions[name] = y_pred\n",
        "        \n",
        "        # Calculate metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        \n",
        "        # Calculate additional metrics\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "        residuals = y_test - y_pred\n",
        "        \n",
        "        results[name] = {\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'R²': r2,\n",
        "            'MAPE': mape,\n",
        "            'Residuals': residuals\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n{name} Model Performance:\")\n",
        "        print(f\"  MSE:  {mse:.2f}\")\n",
        "        print(f\"  RMSE: {rmse:.2f}\")\n",
        "        print(f\"  MAE:  {mae:.2f}\")\n",
        "        print(f\"  R²:   {r2:.4f}\")\n",
        "        print(f\"  MAPE: {mape:.2f}%\")\n",
        "    \n",
        "    return results, predictions\n",
        "\n",
        "def create_evaluation_visualizations(results, predictions, y_test, X_test=None):\n",
        "    \"\"\"Create comprehensive evaluation visualizations\"\"\"\n",
        "    \n",
        "    # 1. Model Performance Comparison\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Performance metrics comparison\n",
        "    metrics_df = pd.DataFrame({\n",
        "        model: {metric: values[metric] for metric in ['MSE', 'RMSE', 'MAE', 'R²', 'MAPE']}\n",
        "        for model, values in results.items()\n",
        "    }).T\n",
        "    \n",
        "    # Plot MSE and RMSE\n",
        "    axes[0,0].bar(metrics_df.index, metrics_df['MSE'], alpha=0.7, color='skyblue')\n",
        "    axes[0,0].set_title('Mean Squared Error (MSE)')\n",
        "    axes[0,0].set_ylabel('MSE')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot R² Score\n",
        "    axes[0,1].bar(metrics_df.index, metrics_df['R²'], alpha=0.7, color='lightgreen')\n",
        "    axes[0,1].set_title('R² Score')\n",
        "    axes[0,1].set_ylabel('R² Score')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot MAE\n",
        "    axes[1,0].bar(metrics_df.index, metrics_df['MAE'], alpha=0.7, color='salmon')\n",
        "    axes[1,0].set_title('Mean Absolute Error (MAE)')\n",
        "    axes[1,0].set_ylabel('MAE')\n",
        "    axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot MAPE\n",
        "    axes[1,1].bar(metrics_df.index, metrics_df['MAPE'], alpha=0.7, color='gold')\n",
        "    axes[1,1].set_title('Mean Absolute Percentage Error (MAPE)')\n",
        "    axes[1,1].set_ylabel('MAPE (%)')\n",
        "    axes[1,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 2. Prediction vs Actual plots\n",
        "    n_models = len(predictions)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, (model_name, y_pred) in enumerate(predictions.items()):\n",
        "        axes[idx].scatter(y_test, y_pred, alpha=0.6)\n",
        "        axes[idx].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        axes[idx].set_xlabel('Actual Charges')\n",
        "        axes[idx].set_ylabel('Predicted Charges')\n",
        "        axes[idx].set_title(f'{model_name}: Predicted vs Actual')\n",
        "        \n",
        "        # Add R² to plot\n",
        "        r2 = results[model_name]['R²']\n",
        "        axes[idx].text(0.05, 0.95, f'R² = {r2:.4f}', transform=axes[idx].transAxes, \n",
        "                      bbox=dict(boxstyle=\"round\", facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # 3. Residual Analysis\n",
        "    fig, axes = plt.subplots(2, n_models, figsize=(6*n_models, 10))\n",
        "    if n_models == 1:\n",
        "        axes = axes.reshape(-1, 1)\n",
        "    \n",
        "    for idx, (model_name, y_pred) in enumerate(predictions.items()):\n",
        "        residuals = results[model_name]['Residuals']\n",
        "        \n",
        "        # Residuals vs Predicted\n",
        "        axes[0, idx].scatter(y_pred, residuals, alpha=0.6)\n",
        "        axes[0, idx].axhline(y=0, color='r', linestyle='--')\n",
        "        axes[0, idx].set_xlabel('Predicted Charges')\n",
        "        axes[0, idx].set_ylabel('Residuals')\n",
        "        axes[0, idx].set_title(f'{model_name}: Residuals vs Predicted')\n",
        "        \n",
        "        # Q-Q plot for residuals normality\n",
        "        stats.probplot(residuals, dist=\"norm\", plot=axes[1, idx])\n",
        "        axes[1, idx].set_title(f'{model_name}: Q-Q Plot (Residuals)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def bias_analysis_evaluation(predictions, y_test, X_test, df_original=None):\n",
        "    \"\"\"\n",
        "    Analyze bias in model predictions across demographic groups\n",
        "    Requires the original dataframe with categorical variables\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"BIAS ANALYSIS IN MODEL PREDICTIONS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if df_original is not None:\n",
        "        # Create analysis dataframe\n",
        "        bias_analysis_df = X_test.copy()\n",
        "        bias_analysis_df['actual_charges'] = y_test\n",
        "        \n",
        "        # Add predictions from each model\n",
        "        for model_name, y_pred in predictions.items():\n",
        "            bias_analysis_df[f'{model_name}_pred'] = y_pred\n",
        "            bias_analysis_df[f'{model_name}_error'] = y_test - y_pred\n",
        "            bias_analysis_df[f'{model_name}_abs_error'] = np.abs(y_test - y_pred)\n",
        "        \n",
        "        # If original demographic data is available, merge it\n",
        "        # This would need to be adapted based on the actual data structure\n",
        "        \n",
        "        # Example bias analysis by age groups\n",
        "        if 'age' in bias_analysis_df.columns:\n",
        "            bias_analysis_df['age_group'] = pd.cut(bias_analysis_df['age'], \n",
        "                                                 bins=[0, 30, 45, 60, 100], \n",
        "                                                 labels=['Young', 'Middle', 'Senior', 'Elderly'])\n",
        "            \n",
        "            print(\"\\nBias Analysis by Age Group:\")\n",
        "            for model_name in predictions.keys():\n",
        "                print(f\"\\n{model_name} - Mean Absolute Error by Age Group:\")\n",
        "                age_bias = bias_analysis_df.groupby('age_group')[f'{model_name}_abs_error'].mean()\n",
        "                print(age_bias)\n",
        "        \n",
        "        return bias_analysis_df\n",
        "    else:\n",
        "        print(\"Original dataframe not available. Cannot perform detailed bias analysis.\")\n",
        "        print(\"Please ensure df_original parameter includes demographic variables.\")\n",
        "\n",
        "# =============================================================================\n",
        "# MOCK EVALUATION FOR TESTING (Remove when real models are available)\n",
        "# =============================================================================\n",
        "\n",
        "def create_mock_evaluation_demo():\n",
        "    \"\"\"\n",
        "    Create a demonstration with mock data\n",
        "    Remove this function when real models from Section 4 are available\n",
        "    \"\"\"\n",
        "    print(\"MOCK EVALUATION DEMONSTRATION\")\n",
        "    print(\"This will be replaced with real model evaluation\")\n",
        "    \n",
        "    # Generate mock data similar to insurance dataset\n",
        "    np.random.seed(42)\n",
        "    n_samples = 200\n",
        "    \n",
        "    # Mock test data\n",
        "    X_test_mock = pd.DataFrame({\n",
        "        'age': np.random.randint(18, 65, n_samples),\n",
        "        'bmi': np.random.normal(25, 5, n_samples),\n",
        "        'children': np.random.randint(0, 5, n_samples),\n",
        "        'sex_encoded': np.random.randint(0, 2, n_samples),\n",
        "        'smoker_encoded': np.random.randint(0, 2, n_samples)\n",
        "    })\n",
        "    \n",
        "    # Mock true charges (with realistic patterns)\n",
        "    y_test_mock = (X_test_mock['age'] * 100 + \n",
        "                   X_test_mock['bmi'] * 200 + \n",
        "                   X_test_mock['smoker_encoded'] * 20000 + \n",
        "                   np.random.normal(0, 2000, n_samples))\n",
        "    \n",
        "    # Mock model predictions (with different accuracy levels)\n",
        "    mock_predictions = {\n",
        "        'Linear_Regression': y_test_mock + np.random.normal(0, 3000, n_samples),\n",
        "        'Random_Forest': y_test_mock + np.random.normal(0, 2000, n_samples),\n",
        "        'Gradient_Boosting': y_test_mock + np.random.normal(0, 1500, n_samples)\n",
        "    }\n",
        "    \n",
        "    # Create mock models dictionary (just for structure)\n",
        "    mock_models = {}\n",
        "    for name in mock_predictions.keys():\n",
        "        class MockModel:\n",
        "            def __init__(self, predictions):\n",
        "                self.predictions = predictions\n",
        "            def predict(self, X):\n",
        "                return self.predictions\n",
        "        mock_models[name] = MockModel(mock_predictions[name])\n",
        "    \n",
        "    # Run evaluation\n",
        "    results, predictions = comprehensive_model_evaluation(mock_models, X_test_mock, y_test_mock)\n",
        "    create_evaluation_visualizations(results, predictions, y_test_mock, X_test_mock)\n",
        "    bias_analysis_df = bias_analysis_evaluation(predictions, y_test_mock, X_test_mock)\n",
        "    \n",
        "    return results, predictions, bias_analysis_df\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EVALUATION EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "# TODO: Replace this mock demonstration with real evaluation once Section 4 is complete\n",
        "print(\"Setting up Model Evaluation Framework...\")\n",
        "print(\"Run create_mock_evaluation_demo() to test the evaluation system\")\n",
        "\n",
        "# When Section 4 is complete, replace the above with:\n",
        "# results, predictions = comprehensive_model_evaluation(trained_models, X_test, y_test)\n",
        "# create_evaluation_visualizations(results, predictions, y_test, X_test)\n",
        "# bias_analysis_df = bias_analysis_evaluation(predictions, y_test, X_test, df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKFDH4LZVQrZ"
      },
      "source": [
        "# Advanced\n",
        "# **Section6: Module Hypertuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SECTION 6: HYPERPARAMETER TUNING ((Alex) I googled this one)\n",
        "# Focus: Optimize model performance and analyze impact on bias\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.svm import SVR\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# HYPERPARAMETER TUNING FRAMEWORK\n",
        "# =============================================================================\n",
        "\n",
        "class HyperparameterTuner:\n",
        "    \"\"\"\n",
        "    Comprehensive hyperparameter tuning class for regression models\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, X_train, y_train, X_test, y_test, cv_folds=5, random_state=42):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        self.cv_folds = cv_folds\n",
        "        self.random_state = random_state\n",
        "        self.tuned_models = {}\n",
        "        self.tuning_results = {}\n",
        "    \n",
        "    def tune_random_forest(self, method='grid', n_iter=20):\n",
        "        \"\"\"Tune Random Forest hyperparameters\"\"\"\n",
        "        \n",
        "        print(\"Tuning Random Forest...\")\n",
        "        \n",
        "        # Define parameter grid\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200, 300],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "        \n",
        "        # Reduced grid for RandomizedSearch\n",
        "        if method == 'random':\n",
        "            param_grid.update({\n",
        "                'n_estimators': [50, 100, 150, 200, 250, 300],\n",
        "                'max_depth': [None, 5, 10, 15, 20, 25, 30],\n",
        "                'min_samples_split': [2, 5, 10, 15],\n",
        "                'min_samples_leaf': [1, 2, 4, 6],\n",
        "            })\n",
        "        \n",
        "        rf = RandomForestRegressor(random_state=self.random_state)\n",
        "        \n",
        "        # Choose search method\n",
        "        if method == 'grid':\n",
        "            search = GridSearchCV(rf, param_grid, cv=self.cv_folds, \n",
        "                                scoring='r2', n_jobs=-1, verbose=1)\n",
        "        else:\n",
        "            search = RandomizedSearchCV(rf, param_grid, n_iter=n_iter, \n",
        "                                      cv=self.cv_folds, scoring='r2', \n",
        "                                      n_jobs=-1, verbose=1, random_state=self.random_state)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        search.fit(self.X_train, self.y_train)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Store results\n",
        "        self.tuned_models['Random_Forest'] = search.best_estimator_\n",
        "        self.tuning_results['Random_Forest'] = {\n",
        "            'best_params': search.best_params_,\n",
        "            'best_score': search.best_score_,\n",
        "            'training_time': end_time - start_time,\n",
        "            'method': method\n",
        "        }\n",
        "        \n",
        "        print(f\"Best RF parameters: {search.best_params_}\")\n",
        "        print(f\"Best CV R² score: {search.best_score_:.4f}\")\n",
        "        print(f\"Tuning time: {end_time - start_time:.2f} seconds\")\n",
        "        \n",
        "        return search.best_estimator_\n",
        "    \n",
        "    def tune_gradient_boosting(self, method='grid', n_iter=20):\n",
        "        \"\"\"Tune Gradient Boosting hyperparameters\"\"\"\n",
        "        \n",
        "        print(\"\\nTuning Gradient Boosting...\")\n",
        "        \n",
        "        # Define parameter grid\n",
        "        param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4]\n",
        "        }\n",
        "        \n",
        "        # Expanded grid for RandomizedSearch\n",
        "        if method == 'random':\n",
        "            param_grid.update({\n",
        "                'n_estimators': [50, 100, 150, 200, 250, 300],\n",
        "                'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
        "                'max_depth': [3, 4, 5, 6, 7, 8],\n",
        "                'subsample': [0.8, 0.9, 1.0]\n",
        "            })\n",
        "        \n",
        "        gb = GradientBoostingRegressor(random_state=self.random_state)\n",
        "        \n",
        "        # Choose search method\n",
        "        if method == 'grid':\n",
        "            search = GridSearchCV(gb, param_grid, cv=self.cv_folds, \n",
        "                                scoring='r2', n_jobs=-1, verbose=1)\n",
        "        else:\n",
        "            search = RandomizedSearchCV(gb, param_grid, n_iter=n_iter, \n",
        "                                      cv=self.cv_folds, scoring='r2', \n",
        "                                      n_jobs=-1, verbose=1, random_state=self.random_state)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        search.fit(self.X_train, self.y_train)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Store results\n",
        "        self.tuned_models['Gradient_Boosting'] = search.best_estimator_\n",
        "        self.tuning_results['Gradient_Boosting'] = {\n",
        "            'best_params': search.best_params_,\n",
        "            'best_score': search.best_score_,\n",
        "            'training_time': end_time - start_time,\n",
        "            'method': method\n",
        "        }\n",
        "        \n",
        "        print(f\"Best GB parameters: {search.best_params_}\")\n",
        "        print(f\"Best CV R² score: {search.best_score_:.4f}\")\n",
        "        print(f\"Tuning time: {end_time - start_time:.2f} seconds\")\n",
        "        \n",
        "        return search.best_estimator_\n",
        "    \n",
        "    def tune_ridge_regression(self):\n",
        "        \"\"\"Tune Ridge Regression alpha parameter\"\"\"\n",
        "        \n",
        "        print(\"\\nTuning Ridge Regression...\")\n",
        "        \n",
        "        param_grid = {\n",
        "            'alpha': [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
        "        }\n",
        "        \n",
        "        ridge = Ridge(random_state=self.random_state)\n",
        "        search = GridSearchCV(ridge, param_grid, cv=self.cv_folds, \n",
        "                            scoring='r2', n_jobs=-1)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        search.fit(self.X_train, self.y_train)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Store results\n",
        "        self.tuned_models['Ridge'] = search.best_estimator_\n",
        "        self.tuning_results['Ridge'] = {\n",
        "            'best_params': search.best_params_,\n",
        "            'best_score': search.best_score_,\n",
        "            'training_time': end_time - start_time,\n",
        "            'method': 'grid'\n",
        "        }\n",
        "        \n",
        "        print(f\"Best Ridge alpha: {search.best_params_}\")\n",
        "        print(f\"Best CV R² score: {search.best_score_:.4f}\")\n",
        "        \n",
        "        return search.best_estimator_\n",
        "    \n",
        "    def tune_lasso_regression(self):\n",
        "        \"\"\"Tune Lasso Regression alpha parameter\"\"\"\n",
        "        \n",
        "        print(\"\\nTuning Lasso Regression...\")\n",
        "        \n",
        "        param_grid = {\n",
        "            'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]\n",
        "        }\n",
        "        \n",
        "        lasso = Lasso(random_state=self.random_state, max_iter=2000)\n",
        "        search = GridSearchCV(lasso, param_grid, cv=self.cv_folds, \n",
        "                            scoring='r2', n_jobs=-1)\n",
        "        \n",
        "        start_time = time.time()\n",
        "        search.fit(self.X_train, self.y_train)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        # Store results\n",
        "        self.tuned_models['Lasso'] = search.best_estimator_\n",
        "        self.tuning_results['Lasso'] = {\n",
        "            'best_params': search.best_params_,\n",
        "            'best_score': search.best_score_,\n",
        "            'training_time': end_time - start_time,\n",
        "            'method': 'grid'\n",
        "        }\n",
        "        \n",
        "        print(f\"Best Lasso alpha: {search.best_params_}\")\n",
        "        print(f\"Best CV R² score: {search.best_score_:.4f}\")\n",
        "        \n",
        "        return search.best_estimator_\n",
        "    \n",
        "    def tune_all_models(self, search_method='random'):\n",
        "        \"\"\"Tune all models\"\"\"\n",
        "        \n",
        "        print(\"=\"*60)\n",
        "        print(\"COMPREHENSIVE HYPERPARAMETER TUNING\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Tune each model\n",
        "        self.tune_random_forest(method=search_method)\n",
        "        self.tune_gradient_boosting(method=search_method)\n",
        "        self.tune_ridge_regression()\n",
        "        self.tune_lasso_regression()\n",
        "        \n",
        "        return self.tuned_models, self.tuning_results\n",
        "    \n",
        "    def compare_tuning_results(self):\n",
        "        \"\"\"Compare tuning results across models\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"HYPERPARAMETER TUNING RESULTS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        results_df = pd.DataFrame({\n",
        "            'Model': list(self.tuning_results.keys()),\n",
        "            'Best_CV_R2': [results['best_score'] for results in self.tuning_results.values()],\n",
        "            'Tuning_Time_Seconds': [results['training_time'] for results in self.tuning_results.values()],\n",
        "            'Method': [results['method'] for results in self.tuning_results.values()]\n",
        "        }).sort_values('Best_CV_R2', ascending=False)\n",
        "        \n",
        "        print(results_df.to_string(index=False))\n",
        "        \n",
        "        # Visualize results\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # CV R² scores\n",
        "        ax1.bar(results_df['Model'], results_df['Best_CV_R2'], alpha=0.7, color='lightblue')\n",
        "        ax1.set_title('Best Cross-Validation R² Scores')\n",
        "        ax1.set_ylabel('R² Score')\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Tuning times\n",
        "        ax2.bar(results_df['Model'], results_df['Tuning_Time_Seconds'], alpha=0.7, color='lightcoral')\n",
        "        ax2.set_title('Hyperparameter Tuning Time')\n",
        "        ax2.set_ylabel('Time (seconds)')\n",
        "        ax2.tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return results_df\n",
        "    \n",
        "    def evaluate_tuned_models(self):\n",
        "        \"\"\"Evaluate all tuned models on test set\"\"\"\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"TUNED MODELS EVALUATION ON TEST SET\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        test_results = {}\n",
        "        \n",
        "        for name, model in self.tuned_models.items():\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(self.X_test)\n",
        "            \n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(self.y_test, y_pred)\n",
        "            r2 = r2_score(self.y_test, y_pred)\n",
        "            mae = mean_absolute_error(self.y_test, y_pred)\n",
        "            \n",
        "            test_results[name] = {\n",
        "                'MSE': mse,\n",
        "                'R²': r2,\n",
        "                'MAE': mae\n",
        "            }\n",
        "            \n",
        "            print(f\"\\n{name}:\")\n",
        "            print(f\"  Test R²:  {r2:.4f}\")\n",
        "            print(f\"  Test MSE: {mse:.2f}\")\n",
        "            print(f\"  Test MAE: {mae:.2f}\")\n",
        "        \n",
        "        return test_results\n",
        "\n",
        "# =============================================================================\n",
        "# BIAS ANALYSIS AFTER HYPERPARAMETER TUNING\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_tuning_impact_on_bias(tuned_models, X_test, y_test, original_models=None):\n",
        "    \"\"\"\n",
        "    Analyze if hyperparameter tuning affects bias in predictions\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BIAS ANALYSIS: IMPACT OF HYPERPARAMETER TUNING\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if original_models is not None:\n",
        "        print(\"Comparing bias before and after tuning...\")\n",
        "        \n",
        "        # This would compare predictions from original vs tuned models\n",
        "        # across different demographic groups\n",
        "        \n",
        "        # Example framework (would need actual demographic data):\n",
        "        for model_name in tuned_models.keys():\n",
        "            if model_name in original_models:\n",
        "                print(f\"\\n{model_name} Bias Analysis:\")\n",
        "                \n",
        "                # Get predictions from both models\n",
        "                tuned_pred = tuned_models[model_name].predict(X_test)\n",
        "                original_pred = original_models[model_name].predict(X_test)\n",
        "                \n",
        "                # Calculate prediction differences\n",
        "                pred_diff = np.abs(tuned_pred - original_pred)\n",
        "                print(f\"  Mean prediction difference: {pred_diff.mean():.2f}\")\n",
        "                print(f\"  Max prediction difference: {pred_diff.max():.2f}\")\n",
        "    \n",
        "    else:\n",
        "        print(\"Original models not provided. Cannot compare bias impact.\")\n",
        "\n",
        "# =============================================================================\n",
        "# MOCK DEMONSTRATION FOR TESTING\n",
        "# =============================================================================\n",
        "\n",
        "def create_mock_tuning_demo():\n",
        "    \"\"\"\n",
        "    Create a demonstration with mock data\n",
        "    Remove this function when real data from Section 3 is available\n",
        "    \"\"\"\n",
        "    print(\"MOCK HYPERPARAMETER TUNING DEMONSTRATION\")\n",
        "    print(\"This will be replaced with real data from Section 3\")\n",
        "    \n",
        "    # Generate mock data\n",
        "    np.random.seed(42)\n",
        "    n_train, n_test = 800, 200\n",
        "    n_features = 5\n",
        "    \n",
        "    X_train_mock = np.random.randn(n_train, n_features)\n",
        "    X_test_mock = np.random.randn(n_test, n_features)\n",
        "    y_train_mock = X_train_mock.sum(axis=1) + np.random.randn(n_train) * 0.1\n",
        "    y_test_mock = X_test_mock.sum(axis=1) + np.random.randn(n_test) * 0.1\n",
        "    \n",
        "    # Convert to DataFrame for consistency\n",
        "    feature_names = [f'feature_{i}' for i in range(n_features)]\n",
        "    X_train_mock = pd.DataFrame(X_train_mock, columns=feature_names)\n",
        "    X_test_mock = pd.DataFrame(X_test_mock, columns=feature_names)\n",
        "    \n",
        "    # Initialize tuner\n",
        "    tuner = HyperparameterTuner(X_train_mock, y_train_mock, X_test_mock, y_test_mock)\n",
        "    \n",
        "    # Run tuning (reduced parameters for demo)\n",
        "    print(\"Running reduced hyperparameter tuning for demonstration...\")\n",
        "    tuned_models, tuning_results = tuner.tune_all_models(search_method='random')\n",
        "    \n",
        "    # Compare results\n",
        "    results_df = tuner.compare_tuning_results()\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    test_results = tuner.evaluate_tuned_models()\n",
        "    \n",
        "    return tuner, tuned_models, tuning_results, test_results\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN TUNING EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Setting up Hyperparameter Tuning Framework...\")\n",
        "print(\"Run create_mock_tuning_demo() to test the tuning system\")\n",
        "\n",
        "# TODO: When Section 3 is complete, replace the above with:\n",
        "# tuner = HyperparameterTuner(X_train, y_train, X_test, y_test)\n",
        "# tuned_models, tuning_results = tuner.tune_all_models()\n",
        "# results_df = tuner.compare_tuning_results()\n",
        "# test_results = tuner.evaluate_tuned_models()\n",
        "\n",
        "# TODO: When Section 4 is complete, add bias comparison:\n",
        "# analyze_tuning_impact_on_bias(tuned_models, X_test, y_test, original_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6rHKHn2WDcx"
      },
      "source": [
        "# Suggested Datasets:\n",
        "\n",
        "--\n",
        "\n",
        "\n",
        "### Public Datasets with Download Links\n",
        "\n",
        "These offer more realistic, messy data for deeper experimentation:\n",
        "\n",
        "####  Classification\n",
        "- [**Titanic Dataset**](https://www.kaggle.com/c/titanic/data) – Predict survival based on passenger features.\n",
        "- [**Heart Disease Dataset**](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset) – Binary classification with medical features.\n",
        "- [**Mushroom Classification Dataset**](https://www.kaggle.com/datasets/uciml/mushroom-classification) – classification of mushrooms.\n",
        "- [**Breast Cancer Dataset**](https://www.kaggle.com/datasets/reihanenamdari/breast-cancer) – Binary classification with medical features.\n",
        "- [**Student Alcohol Consumption Dataset**](https://www.kaggle.com/datasets/reihanenamdari/breast-cancer) – multi-classification task.\n",
        "- [**Credit Card Prediction**](https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction) – Classification task to predict if a client can have a credit card.\n",
        "- [**Bank Marketing Dataset**](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) – Predict if a client will subscribe to a term deposit. Includes categorical and temporal features.\n",
        "\n",
        "#### Regression\n",
        "\n",
        "- [**Student Performance Dataset**](https://www.kaggle.com/datasets/spscientist/students-performance-in-exams/data) – Predict final grades based on demographic and school-related features.\n",
        "- [**Medical Cost Dataset**](https://www.kaggle.com/datasets/mirichoi0218/insurance) – Predict insurance charge.\n",
        "- [**Bike Sharing Dataset**](https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset) – Predict the share of bikes.\n",
        "- [**Salary Prediction**](https://www.kaggle.com/datasets/amirmahdiabbootalebi/salary-by-job-title-and-country) – Predict Salary for employers\n",
        "- [**Medical Cost Personal Dataset**](https://www.kaggle.com/datasets/mirichoi0218/insurance) – Predict insurance charges based on age, BMI, smoking status, etc..\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alex Rush, Orlando Igwe, Zakaria Miah, Mohammed Shams"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
